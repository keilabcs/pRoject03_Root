#############  Aprendizagem profunda ################

# O objetivo é treinar um modelo de skip-gram Word2Vec sobre dados do Text8.

#Estes são todos os módulos que usaremos mais tarde. 
#Certifique-se de poder importá-los
#Antes de prosseguir.

%matplotlib inline
from __future__ import print_function
import collections
import math
import numpy as np
import os
import random
import tensorflow as tf
import zipfile
from matplotlib import pylab
from six.moves import range
from six.moves.urllib.request import urlretrieve
from sklearn.manifold import TSNE

import matplotlib.pyplot as plt
from matplotlib import offsetbox
from sklearn import (manifold, datasets, decomposition, ensemble,
             discriminant_analysis, random_projection)
import pandas as pd

## Fazendo o download dos dados do site da fonte.
url = 'http://mattmahoney.net/dc/'

def maybe_download(filename, expected_bytes):
  """Download a file if not present, and make sure it's the right size."""
  if not os.path.exists(filename):
    filename, _ = urlretrieve(url + filename, filename)
  statinfo = os.stat(filename)
  if statinfo.st_size == expected_bytes:
    print('Found and verified %s' % filename)
  else:
    print(statinfo.st_size)
    raise Exception(
      'Failed to verify ' + filename + '. Can you get to it with a browser?')
  return filename
  
#Encontrada e verificada text8.zip
filename = maybe_download('text8.zip', 31344016) 


#Leia os dados em uma string.
def read_data(filename):
  """Extraia o primeiro arquivo encerrado em um arquivo zip como uma lista de palavras"""
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
  return data
  
words = read_data(filename)
print('Data size %d' % len(words))

#Tamanho dos dados 17005207
#Crie o dicionário e substitua palavras raras por um token UNK.
vocabulary_size = 50000

def build_dataset(words):
  count = [['UNK', -1]]
  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
  dictionary = dict()
  for word, _ in count:
    dictionary[word] = len(dictionary)
  data = list()
  unk_count = 0
  for word in words:
    if word in dictionary:
      index = dictionary[word]
    else:
      index = 0  # dictionary['UNK']
      unk_count = unk_count + 1
    data.append(index)
  count[0][1] = unk_count
  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
  return data, count, dictionary, reverse_dictionary

data, count, dictionary, reverse_dictionary = build_dataset(words)
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10])
del words  # Sugestão para reduzir a memória.

#Palavras mais comuns (+ UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('e', 416629), ('one', 411764)]
#Dados de amostra [5243, 3083, 12, 6, 195, 2, 3136, 46, 59, 156]

#Função para gerar um lote de treinamento para o modelo skip-gram.
data_index = 0

def generate_batch(batch_size, num_skips, skip_window):
  global data_index
  assert batch_size % num_skips == 0
  assert num_skips <= 2 * skip_window
  batch = np.ndarray(shape=(batch_size), dtype=np.int32)
  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
  span = 2 * skip_window + 1 # [ skip_window target skip_window ]
  buffer = collections.deque(maxlen=span)
  for _ in range(span):
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)
  for i in range(batch_size // num_skips):
    target = skip_window  # target label at the center of the buffer
    targets_to_avoid = [ skip_window ]
    for j in range(num_skips):
      while target in targets_to_avoid:
        target = random.randint(0, span - 1)
      targets_to_avoid.append(target)
      batch[i * num_skips + j] = buffer[skip_window]
      labels[i * num_skips + j, 0] = buffer[target]
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)
  return batch, labels

print('data:', [reverse_dictionary[di] for di in data[:8]])

for num_skips, skip_window in [(2, 1), (4, 2)]:
    data_index = 0
    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
    print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
    print('    batch:', [reverse_dictionary[bi] for bi in batch])
    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])


#data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']

#with num_skips = 2 and skip_window = 1:
    #batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']
    #labels: ['as', 'anarchism', 'a', 'originated', 'term', 'as', 'a', 'of']

#with num_skips = 4 and skip_window = 2:
    #batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']
    #labels: ['anarchism', 'originated', 'term', 'a', 'as', 'of', 'originated', 'term']

################Treine um modelo de skip-gram.
batch_size = 128
embedding_size = 128 # Dimensão do vetor embedding.
skip_window = 1 # Quantas palavras a considerar esquerda e direita.
num_skips = 2 # Quantas vezes reutilizar uma entrada para gerar uma etiqueta.
# Escolhemos um conjunto de validação aleatória para amostrar vizinhos mais próximos. Aqui limitamos
# Amostras de validação para as palavras que têm uma identificação numérica baixa, que por
# Construção também são as mais freqüentes.
valid_size = 16 # Conjunto aleatório de palavras para avaliar similaridade.
valid_window = 100 #Escolha apenas amostras de dev na cabeça da distribuição.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64 # Número de exemplos negativos a serem amostrados.

graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):

  # Input data.
  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
  
  # Variables.
  embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
  softmax_weights = tf.Variable(
    tf.truncated_normal([vocabulary_size, embedding_size],
                        stddev=1.0 / math.sqrt(embedding_size)))
  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
  
  # Model.
  # Look up embeddings for inputs.
  embed = tf.nn.embedding_lookup(embeddings, train_dataset)
  # Compute the softmax loss, using a sample of the negative labels each time.
  loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))

# Otimizador.
# Nota: o otimizador otimizará os softmax_weights AND the embeddings.
# Isso ocorre porque os embeddings são definidos como uma quantidade variável e o
# O método `minimizar 'do otimizador # modificará, por padrão, todas as quantidades variáveis
# Que contribuem para o tensor que é passado.
# Consulte os documentos no `tf.train.Optimizer.minimize ()` para obter mais detalhes.
  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
  
# Calcule a semelhança entre os exemplos minibatch e todos os embeddings.
# Usamos a distância coseno:
  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
  normalized_embeddings = embeddings / norm
  valid_embeddings = tf.nn.embedding_lookup(
    normalized_embeddings, valid_dataset)
  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))

num_steps = 100001

with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print('Initialized')
  average_loss = 0
  for step in range(num_steps):
    batch_data, batch_labels = generate_batch(
      batch_size, num_skips, skip_window)
    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
    _, l = session.run([optimizer, loss], feed_dict=feed_dict)
    average_loss += l
    if step % 2000 == 0:
      if step > 0:
        average_loss = average_loss / 2000
     # A perda média é uma estimativa da perda nos últimos 2000 lotes.
      print('Average loss at step %d: %f' % (step, average_loss))
      average_loss = 0
     # Note que isso é caro (~ 20% de desaceleração se computado a cada 500 passos)
    if step % 10000 == 0:
      sim = similarity.eval()
      for i in range(valid_size):
        valid_word = reverse_dictionary[valid_examples[i]]
        top_k = 8 # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k+1]
        log = 'Nearest to %s:' % valid_word
        for k in range(top_k):
          close_word = reverse_dictionary[nearest[k]]
          log = '%s %s,' % (log, close_word)
        print(log)
  final_embeddings = normalized_embeddings.eval()
  

### TSNE

num_points = 400
tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])

def plot(embeddings, labels):
  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
  pylab.figure(figsize=(15,15))  # in inches
  for i, label in enumerate(labels):
    x, y = embeddings[i,:]
    pylab.scatter(x, y)
    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                   ha='right', va='bottom')
  pylab.show()
  
# PLOT GRAFICO
words = [reverse_dictionary[i] for i in range(1, num_points+1)]
plot(two_d_embeddings, words)


#Uma alternativa para skip-gram é outro modelo Word2Vec chamado 
#CBOW (Continuous Bag of Words). No modelo CBOW, em vez de prever uma palavra de contexto
#a partir de um vetor de palavras, você prevê uma palavra da soma de todos os vetores da 
#palavra em seu contexto. Implementar e avaliar um modelo CBOW treinado no conjunto de 
#dados text8.

